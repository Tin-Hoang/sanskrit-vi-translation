defaults:
  - _self_

# CLI Arguments / General Settings
input: null

no_cache: false
clear_cache: false
limit: null

# Batch sizes
batch_size: 20  # Default for translation

# Model Configuration
judge:
  model: "gemini/gemini-3-flash-preview"
  temperature: 1.0
  batch_size: 30

translator:
  models:
    - id: "groq/openai/gpt-oss-20b"
      name: "GPT-OSS-20B"
      temperature: 0.3
    - id: "groq/openai/gpt-oss-120b"
      name: "GPT-OSS-120b"
      temperature: 0.3
    - id: "groq/llama-3.1-8b-instant"
      name: "Llama-3.1-8b"
      temperature: 0.3
    - id: "groq/llama-3.3-70b-versatile"
      name: "Llama-3.3-70B"
      temperature: 0.3
    - id: "groq/meta-llama/llama-4-maverick-17b-128e-instruct"
      name: "Llama-4-Maverick-17B"
      temperature: 0.3
    - id: "groq/moonshotai/kimi-k2-instruct-0905"
      name: "Kimi-K2"
      temperature: 0.3
    - id: "groq/qwen/qwen3-32b"
      name: "Qwen3-32B"
      temperature: 0.3
    - id: "gemini/gemini-3-flash-preview"
      name: "Gemini-3-Flash"
      temperature: 1.0
    # Example for Local vLLM (OpenAI Compatible)
    # - id: "openai/Qwen/Qwen2.5-7B-Instruct"
    #   name: "vLLM-Qwen2.5-7B"
    #   api_base: "http://localhost:8000/v1"
    #   api_key: "EMPTY"
    #   temperature: 0.3
  defaults:
    temperature: 0.3
    batch_size: 10
